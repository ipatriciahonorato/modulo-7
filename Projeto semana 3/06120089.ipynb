{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a613fcb7-24db-4244-8ad4-d2b27681087a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "# Importação Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "370f52c3-4af6-491f-9f2c-81a93cd872da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\python310\\lib\\site-packages (12.0.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from pyarrow) (1.24.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastparquet in c:\\python310\\lib\\site-packages (2023.7.0)\n",
      "Requirement already satisfied: pandas>=1.5.0 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from fastparquet) (2.0.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from fastparquet) (1.24.3)\n",
      "Requirement already satisfied: cramjam>=2.3 in c:\\python310\\lib\\site-packages (from fastparquet) (2.7.0)\n",
      "Requirement already satisfied: fsspec in c:\\python310\\lib\\site-packages (from fastparquet) (2023.6.0)\n",
      "Requirement already satisfied: packaging in c:\\python310\\lib\\site-packages (from fastparquet) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python310\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python310\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (2.0.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python310\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (1.24.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from seaborn) (1.24.3)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from seaborn) (2.0.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from seaborn) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\python310\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python310\\lib\\site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python310\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\python310\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\python310\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\python310\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python310\\lib\\site-packages (from scikit-learn) (3.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement scipy.stats (from versions: none)\n",
      "ERROR: No matching distribution found for scipy.stats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask in c:\\python310\\lib\\site-packages (2023.8.1)\n",
      "Requirement already satisfied: click>=8.0 in c:\\python310\\lib\\site-packages (from dask) (8.1.3)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in c:\\python310\\lib\\site-packages (from dask) (2.2.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in c:\\python310\\lib\\site-packages (from dask) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python310\\lib\\site-packages (from dask) (23.1)\n",
      "Requirement already satisfied: partd>=1.2.0 in c:\\python310\\lib\\site-packages (from dask) (1.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\python310\\lib\\site-packages (from dask) (6.0)\n",
      "Requirement already satisfied: toolz>=0.10.0 in c:\\python310\\lib\\site-packages (from dask) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in c:\\python310\\lib\\site-packages (from dask) (6.8.0)\n",
      "Requirement already satisfied: pandas>=1.3 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from dask) (2.0.1)\n",
      "Requirement already satisfied: colorama in c:\\python310\\lib\\site-packages (from click>=8.0->dask) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\python310\\lib\\site-packages (from importlib-metadata>=4.13.0->dask) (3.16.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python310\\lib\\site-packages (from pandas>=1.3->dask) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas>=1.3->dask) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python310\\lib\\site-packages (from pandas>=1.3->dask) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from pandas>=1.3->dask) (1.24.3)\n",
      "Requirement already satisfied: locket in c:\\python310\\lib\\site-packages (from partd>=1.2.0->dask) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3->dask) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tslearn in c:\\python310\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from tslearn) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from tslearn) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\python310\\lib\\site-packages (from tslearn) (1.3.0)\n",
      "Requirement already satisfied: numba in c:\\python310\\lib\\site-packages (from tslearn) (0.57.1)\n",
      "Requirement already satisfied: joblib in c:\\python310\\lib\\site-packages (from tslearn) (1.3.2)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in c:\\python310\\lib\\site-packages (from numba->tslearn) (0.40.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python310\\lib\\site-packages (from scikit-learn->tslearn) (3.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/ba/7c/b971f2485155917ecdcebb210e021e36a6b65457394590be01cc61515310/tensorflow-2.13.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Using cached tensorflow-2.13.0-cp310-cp310-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting tensorflow-intel==2.13.0 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-intel==2.13.0 from https://files.pythonhosted.org/packages/40/fa/98115f6fe4d92e1962f549917be2dc8e369853b7e404191996fedaaf4dd6/tensorflow_intel-2.13.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Using cached tensorflow_intel-2.13.0-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.5.26)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py>=2.9.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for h5py>=2.9.0 from https://files.pythonhosted.org/packages/e2/c4/6f8dae1530d57a6122fd5b72c750187484acbe612f630cb2179e4bcb12c1/h5py-3.9.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Using cached h5py-3.9.0-cp310-cp310-win_amd64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.24.1)\n",
      "Requirement already satisfied: setuptools in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (58.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.15.0)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/b7/80/56048dd2223a075f421dba172074c93d5545a4348e0699eee15120b4f41d/grpcio-1.57.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Using cached grpcio-1.57.0-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for keras<2.14,>=2.13.1 from https://files.pythonhosted.org/packages/2e/f3/19da7511b45e80216cbbd9467137b2d28919c58ba1ccb971435cb631e470/keras-2.13.1-py3-none-any.whl.metadata\n",
      "  Using cached keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\python310\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.41.1)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/9c/8d/bff87fc722553a5691d8514da5523c23547f3894189ba03b57592e37bdc2/google_auth-2.22.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_auth-2.22.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\python310\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\python310\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\python310\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.2.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.16)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\inteli\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\python310\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n",
      "Using cached tensorflow-2.13.0-cp310-cp310-win_amd64.whl (1.9 kB)\n",
      "Using cached tensorflow_intel-2.13.0-cp310-cp310-win_amd64.whl (276.5 MB)\n",
      "Using cached grpcio-1.57.0-cp310-cp310-win_amd64.whl (4.3 MB)\n",
      "Using cached h5py-3.9.0-cp310-cp310-win_amd64.whl (2.7 MB)\n",
      "Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Installing collected packages: rsa, pyasn1-modules, keras, h5py, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] O sistema não pode encontrar o arquivo especificado: 'C:\\\\Python310\\\\Scripts\\\\pyrsa-decrypt.exe' -> 'C:\\\\Python310\\\\Scripts\\\\pyrsa-decrypt.exe.deleteme'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow\n",
    "!pip install fastparquet\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install seaborn\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install scipy.stats\n",
    "!pip install dask dask[dataframe]\n",
    "!pip install tslearn\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e78eca97-bd2a-445d-b055-758c9e992127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89de489c-1b72-461d-ad10-ff4dce381c73",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Extração dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e13f1a-c762-442e-9c9a-f5891a1acec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo TCRF_ARCHIVE_06120089_20221212212501.parquet tem 6002 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20221212212502.parquet tem 6081 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20221212212504.parquet tem 10301 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20221212212505.parquet tem 41361 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20221212212506.parquet tem 6961 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20221212212507.parquet tem 17721 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20221212212508.parquet tem 45721 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20221212212509.parquet tem 21781 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20221212212510.parquet tem 46661 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20221212212511.parquet tem 28741 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20221212212512.parquet tem 13981 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20221212212513.parquet tem 60010 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20221212212514.parquet tem 7741 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20221212212515.parquet tem 8081 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20221212212516.parquet tem 62001 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20221212212517.parquet tem 12341 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20221212212518.parquet tem 14801 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20221212212519.parquet tem 20601 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20221212212520.parquet tem 9781 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20221212212521.parquet tem 219061 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20221217195137.parquet tem 157841 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20221220151233.parquet tem 108741 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20221223191732.parquet tem 201001 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230103215302.parquet tem 35261 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230103215303.parquet tem 197341 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230104011532.parquet tem 208601 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230115012501.parquet tem 15441 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230115012502.parquet tem 82481 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230115043030.parquet tem 111261 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230115080230.parquet tem 107301 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230115122730.parquet tem 87081 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230115142130.parquet tem 86221 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230115222430.parquet tem 75421 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230116002130.parquet tem 92861 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230116081731.parquet tem 170081 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230116101530.parquet tem 77901 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230116122730.parquet tem 88761 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230116182931.parquet tem 153101 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230116213731.parquet tem 158761 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230116232930.parquet tem 62761 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230117013331.parquet tem 76981 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230117050134.parquet tem 163501 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230117080433.parquet tem 163041 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230117102434.parquet tem 71721 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230117124532.parquet tem 94441 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230117161133.parquet tem 120741 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230117183935.parquet tem 120361 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230117224134.parquet tem 165841 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230118004734.parquet tem 101241 linhas.\n",
      "Arquivo TCRF_ARCHIVE_06120089_20230118122401.parquet tem 11761 linhas.\n",
      "O número total de linhas em todos os arquivos é: 4027580\n"
     ]
    }
   ],
   "source": [
    "def concatenate_and_count_rows_in_parquet_files(file_path, filtered_cols):\n",
    "    total_rows = 0\n",
    "    list_of_dfs = []  # Lista para armazenar os DataFrames lidos\n",
    "    \n",
    "    # Lista de arquivos Parquet no diretório especificado\n",
    "    parquet_files = [file for file in os.listdir(file_path) if file.endswith('.parquet')]\n",
    "    \n",
    "    # Loop para ler cada arquivo Parquet, contar as linhas e armazenar o DataFrame na lista\n",
    "    for file_name in parquet_files:\n",
    "        full_path = os.path.join(file_path, file_name)\n",
    "        df = dd.read_parquet(full_path, columns=filtered_cols)\n",
    "        total_rows += len(df)\n",
    "        list_of_dfs.append(df)\n",
    "        print(f\"Arquivo {file_name} tem {len(df)} linhas.\")\n",
    "    \n",
    "    # Concatenar todos os Dask DataFrames em um único Dask DataFrame\n",
    "    concatenated_df = dd.concat(list_of_dfs, interleave_partitions=True)\n",
    "    \n",
    "    return concatenated_df, total_rows\n",
    "\n",
    "# Uso da função\n",
    "file_path = \"primeiro_arquivo_zip/\"\n",
    "filtered_cols = ['recording_time', 'dateDay-1', 'dateMonth-1', 'dateYear-1', 'phaseOfFlight-1',\n",
    "                 'message0418DAA-1','message0422DAA-1','amscHprsovDrivF-1a', 'amscHprsovDrivF-1b',\n",
    "                 'amscHprsovDrivF-2b', 'amscPrsovDrivF-1a',\n",
    "                 'amscPrsovDrivF-1b', 'amscPrsovDrivF-2b',\n",
    "                 'basBleedLowPressF-1a', 'basBleedLowPressF-2b',\n",
    "                 'basBleedLowTempF-1a', 'basBleedLowTempF-2b',\n",
    "                 'basBleedOverPressF-1a', 'basBleedOverPressF-2b',\n",
    "                 'basBleedOverTempF-1a', 'basBleedOverTempF-2b',\n",
    "                 'bleedFavTmCmd-1a', 'bleedFavTmCmd-1b',\n",
    "                 'bleedFavTmCmd-2a', 'bleedFavTmCmd-2b', 'bleedFavTmFbk-1a',\n",
    "                 'bleedFavTmFbk-1b', 'bleedFavTmFbk-2b', 'bleedHprsovCmdStatus-1a',\n",
    "                 'bleedHprsovCmdStatus-1b', 'bleedHprsovCmdStatus-2a',\n",
    "                 'bleedHprsovCmdStatus-2b', 'bleedHprsovOpPosStatus-1a',\n",
    "                 'bleedHprsovOpPosStatus-1b', 'bleedHprsovOpPosStatus-2a',\n",
    "                 'bleedHprsovOpPosStatus-2b', 'bleedMonPress-1a',\n",
    "                 'bleedMonPress-1b', 'bleedMonPress-2a', 'bleedMonPress-2b',\n",
    "                 'bleedOnStatus-1a', 'bleedOnStatus-1b', 'bleedOnStatus-2b',\n",
    "                 'bleedOverpressCas-2a', 'bleedOverpressCas-2b',\n",
    "                 'bleedPrecoolDiffPress-1a', 'bleedPrecoolDiffPress-1b',\n",
    "                 'bleedPrecoolDiffPress-2a', 'bleedPrecoolDiffPress-2b',\n",
    "                 'bleedPrsovClPosStatus-1a', 'bleedPrsovClPosStatus-2a',\n",
    "                 'bleedPrsovFbk-1a']\n",
    "\n",
    "resulting_df, total_rows = concatenate_and_count_rows_in_parquet_files(file_path, filtered_cols)\n",
    "print(f\"O número total de linhas em todos os arquivos é: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00f8f11-5a9d-4078-ba9d-736782669cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b131403d-6b62-4111-8ce6-a75571d56f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c375ddcc-a7a3-4c35-af34-bef86f4aa6a9",
   "metadata": {},
   "source": [
    "# Pré processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf42f93e-c1b7-4d77-969a-9dcfda983b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_except_record_time(dask_df):\n",
    "    \"\"\"\n",
    "    Remove linhas duplicadas com base em todas as colunas, exceto 'record_time'.\n",
    "\n",
    "    Parâmetros:\n",
    "    - dask_df: DataFrame Dask de entrada.\n",
    "\n",
    "    Retorna:\n",
    "    - DataFrame Dask com linhas duplicadas removidas.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Crie uma lista de colunas a serem consideradas ao identificar duplicatas\n",
    "    columns_to_consider = [col for col in dask_df.columns if col != 'record_time']\n",
    "    \n",
    "    # Use o método drop_duplicates do Dask para remover duplicatas com base nas colunas especificadas\n",
    "    #Mantém o primeiro valor duplicado por default\n",
    "    unique_df = dask_df.drop_duplicates(subset=columns_to_consider)\n",
    "    \n",
    "    return unique_df\n",
    "\n",
    "# Uso da função\n",
    "unique_df = remove_duplicates_except_record_time(resulting_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fee3fc64-7315-463c-8216-f98dfe7d94a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de linhas depois da remoção de duplicatas: 1711622\n"
     ]
    }
   ],
   "source": [
    "# Contar o número de linhas em 'unique_df' (depois da remoção de duplicatas)\n",
    "after_row_count = len(unique_df)\n",
    "\n",
    "# Imprimir os resultado\n",
    "print(f\"Número de linhas depois da remoção de duplicatas: {after_row_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8771408-6aca-4206-8d40-16a1cd12b504",
   "metadata": {},
   "source": [
    "### Rotulagem de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07307ecb-dabf-4695-9861-8fb277be925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegando uma amostra de 5% do conjunto de dados\n",
    "sample_df = unique_df.sample(frac=0.05).compute()\n",
    "\n",
    "# Verificando valores únicos na amostra para 'message0418DAA-1'\n",
    "unique_values_0418_sample = sample_df['message0418DAA-1'].unique()\n",
    "print(\"Valores únicos na amostra para 'message0418DAA-1':\", unique_values_0418_sample)\n",
    "\n",
    "# Verificando valores únicos na amostra para 'message0422DAA-1'\n",
    "unique_values_0422_sample = sample_df['message0422DAA-1'].unique()\n",
    "print(\"Valores únicos na amostra para 'message0422DAA-1':\", unique_values_0422_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed2866f8-db68-42fe-a34a-7dabb290078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Criar Coluna de Rótulo\n",
    "def label_rows(row):\n",
    "    if (row['message0418DAA-1'] > 0) or (row['message0422DAA-1'] > 0):\n",
    "        return 'falha'\n",
    "    else:\n",
    "        return 'normal'\n",
    "\n",
    "unique_df = unique_df.map_partitions(lambda df: df.assign(label=df.apply(label_rows, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7855112-cc99-49f5-94ed-090045e7a888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN counts before transformation:\n",
      "dateYear-1     1627737\n",
      "dateMonth-1    1627737\n",
      "dateDay-1      1627737\n",
      "dtype: int64\n",
      "\n",
      "NaN counts after transformation:\n",
      "dateYear-1     0\n",
      "dateMonth-1    0\n",
      "dateDay-1      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 2. Verificar a presença de NaNs antes da transformação\n",
    "nan_counts_before = unique_df[['dateYear-1', 'dateMonth-1', 'dateDay-1']].isna().sum().compute()\n",
    "print(\"NaN counts before transformation:\")\n",
    "print(nan_counts_before)\n",
    "\n",
    "# 3. Tratar os NaNs\n",
    "unique_df = unique_df.fillna({'dateYear-1': 0, 'dateMonth-1': 0, 'dateDay-1': 0})\n",
    "\n",
    "# 4. Função para criar a coluna 'full_date'\n",
    "def create_full_date(row):\n",
    "    year = str(int(row['dateYear-1']))\n",
    "    month = str(int(row['dateMonth-1'])).zfill(2)  # Garante que o mês tenha 2 dígitos\n",
    "    day = str(int(row['dateDay-1'])).zfill(2)  # Garante que o dia tenha 2 dígitos\n",
    "    return f\"{year}-{month}-{day}\"\n",
    "\n",
    "unique_df = unique_df.map_partitions(lambda df: df.assign(full_date=df.apply(create_full_date, axis=1)))\n",
    "\n",
    "# 5. Verificar a presença de NaNs após a transformação\n",
    "nan_counts_after = unique_df[['dateYear-1', 'dateMonth-1', 'dateDay-1']].isna().sum().compute()\n",
    "print(\"\\nNaN counts after transformation:\")\n",
    "print(nan_counts_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3ca340-43d8-4053-b95a-d0a2db9470bd",
   "metadata": {},
   "source": [
    "### Extração de características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ede783-fb39-4841-b451-32be989f09bc",
   "metadata": {},
   "source": [
    "#### Regressão logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdc34105-3d8b-44b6-b2b7-5c68b8becd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de colunas que são fluxos de sensor\n",
    "sensor_columns = [col for col in unique_df.columns if col not in ['label', 'full_date']]\n",
    "\n",
    "# Calcular estatísticas básicas\n",
    "stats_df = unique_df[sensor_columns].describe().compute()\n",
    "\n",
    "# Se você quiser adicionar mais características, pode fazê-lo aqui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3afb0ed-0e2c-4da6-91bf-bb3de1e03609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo o DataFrame Dask para Pandas\n",
    "unique_df_pd = unique_df.compute()\n",
    "\n",
    "X = unique_df_pd.drop(columns=['label', 'full_date'])\n",
    "y = unique_df_pd['label']\n",
    "\n",
    "# Dividindo os dados\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1df0932-255d-4c6f-84b3-eae564fcbd54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=10000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=10000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Criar um imputador que substitui NaN por 0\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "\n",
    "# Ajustar o imputador aos dados de treinamento e transformá-los\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "\n",
    "# Transformar os dados de teste usando o mesmo imputador\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Agora, treine o modelo LogisticRegression nos dados imputados\n",
    "clf.fit(X_train_imputed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22a676dc-f770-4395-861f-38545c37c3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia no conjunto de teste: 0.9996\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred = clf.predict(X_test_imputed)\n",
    "\n",
    "# Calcular a acurácia\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Acurácia no conjunto de teste: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f14942e3-050a-47f0-ad5f-fecc413e943f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['recording_time', 'dateDay-1', 'dateMonth-1', 'dateYear-1',\n",
      "       'phaseOfFlight-1', 'message0418DAA-1', 'message0422DAA-1',\n",
      "       'amscHprsovDrivF-1a', 'amscHprsovDrivF-1b', 'amscHprsovDrivF-2b',\n",
      "       'amscPrsovDrivF-1a', 'amscPrsovDrivF-1b', 'amscPrsovDrivF-2b',\n",
      "       'basBleedLowPressF-1a', 'basBleedLowPressF-2b', 'basBleedLowTempF-1a',\n",
      "       'basBleedLowTempF-2b', 'basBleedOverPressF-1a', 'basBleedOverPressF-2b',\n",
      "       'basBleedOverTempF-1a', 'basBleedOverTempF-2b', 'bleedFavTmCmd-1a',\n",
      "       'bleedFavTmCmd-1b', 'bleedFavTmCmd-2a', 'bleedFavTmCmd-2b',\n",
      "       'bleedFavTmFbk-1a', 'bleedFavTmFbk-1b', 'bleedFavTmFbk-2b',\n",
      "       'bleedHprsovCmdStatus-1a', 'bleedHprsovCmdStatus-1b',\n",
      "       'bleedHprsovCmdStatus-2a', 'bleedHprsovCmdStatus-2b',\n",
      "       'bleedHprsovOpPosStatus-1a', 'bleedHprsovOpPosStatus-1b',\n",
      "       'bleedHprsovOpPosStatus-2a', 'bleedHprsovOpPosStatus-2b',\n",
      "       'bleedMonPress-1a', 'bleedMonPress-1b', 'bleedMonPress-2a',\n",
      "       'bleedMonPress-2b', 'bleedOnStatus-1a', 'bleedOnStatus-1b',\n",
      "       'bleedOnStatus-2b', 'bleedOverpressCas-2a', 'bleedOverpressCas-2b',\n",
      "       'bleedPrecoolDiffPress-1a', 'bleedPrecoolDiffPress-1b',\n",
      "       'bleedPrecoolDiffPress-2a', 'bleedPrecoolDiffPress-2b',\n",
      "       'bleedPrsovClPosStatus-1a', 'bleedPrsovClPosStatus-2a',\n",
      "       'bleedPrsovFbk-1a', 'label', 'full_date'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(unique_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b47bd2d8-05de-4ba2-b581-fcc4d2bef39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                mean   median        std\n",
      "amscHprsovDrivF-1a          0.000000   0.0000   0.000000\n",
      "amscHprsovDrivF-1b          0.000000   0.0000   0.000000\n",
      "amscHprsovDrivF-2b          0.000012   0.0000   0.003453\n",
      "amscPrsovDrivF-1a           0.000000   0.0000   0.000000\n",
      "amscPrsovDrivF-1b           0.000000   0.0000   0.000000\n",
      "amscPrsovDrivF-2b           0.000000   0.0000   0.000000\n",
      "basBleedLowPressF-1a        0.000000   0.0000   0.000000\n",
      "basBleedLowPressF-2b        0.000000   0.0000   0.000000\n",
      "basBleedLowTempF-1a         0.000000   0.0000   0.000000\n",
      "basBleedLowTempF-2b         0.000000   0.0000   0.000000\n",
      "basBleedOverPressF-1a       0.000000   0.0000   0.000000\n",
      "basBleedOverPressF-2b       0.000000   0.0000   0.000000\n",
      "basBleedOverTempF-1a        0.000000   0.0000   0.000000\n",
      "basBleedOverTempF-2b        0.003573   0.0000   0.059669\n",
      "bleedFavTmCmd-1a           31.799998   0.0000  53.046006\n",
      "bleedFavTmCmd-1b           65.930241  95.0000  56.857361\n",
      "bleedFavTmCmd-2a           28.556560   0.0000  59.402012\n",
      "bleedFavTmCmd-2b           28.067827   0.0000  61.694089\n",
      "bleedFavTmFbk-1a           33.292159   0.6250  54.290678\n",
      "bleedFavTmFbk-1b           68.374990  98.7500  58.406339\n",
      "bleedFavTmFbk-2b           29.348566   0.3750  62.948486\n",
      "bleedHprsovCmdStatus-1a     0.281155   0.0000   0.449564\n",
      "bleedHprsovCmdStatus-1b     0.329690   0.0000   0.470101\n",
      "bleedHprsovCmdStatus-2a     0.265254   0.0000   0.441469\n",
      "bleedHprsovCmdStatus-2b     0.265257   0.0000   0.441471\n",
      "bleedHprsovOpPosStatus-1a   0.618743   1.0000   0.485696\n",
      "bleedHprsovOpPosStatus-1b   0.618544   1.0000   0.485745\n",
      "bleedHprsovOpPosStatus-2a   0.578968   1.0000   0.493725\n",
      "bleedHprsovOpPosStatus-2b   0.578981   1.0000   0.493723\n",
      "bleedMonPress-1a           64.070120  49.5000  36.549209\n",
      "bleedMonPress-1b           63.987105  49.3750  36.532343\n",
      "bleedMonPress-2a           63.320071  50.2500  48.534772\n",
      "bleedMonPress-2b           62.830814  50.2500  51.645738\n",
      "bleedOnStatus-1a            0.869372   1.0000   0.336993\n",
      "bleedOnStatus-1b            0.869370   1.0000   0.336995\n",
      "bleedOnStatus-2b            0.827405   1.0000   0.377897\n",
      "bleedOverpressCas-2a        0.000000   0.0000   0.000000\n",
      "bleedOverpressCas-2b        0.000000   0.0000   0.000000\n",
      "bleedPrecoolDiffPress-1a    0.579703   0.6250   0.337032\n",
      "bleedPrecoolDiffPress-1b    0.580051   0.6250   0.336933\n",
      "bleedPrecoolDiffPress-2a    0.123749   0.5625   7.092480\n",
      "bleedPrecoolDiffPress-2b    0.001630   0.5625   8.120509\n",
      "bleedPrsovClPosStatus-1a    0.130635   0.0000   0.337001\n",
      "bleedPrsovClPosStatus-2a    0.161958   0.0000   0.368412\n",
      "bleedPrsovFbk-1a           44.846309   0.8750  72.434188\n"
     ]
    }
   ],
   "source": [
    "# Lista de colunas para calcular as características\n",
    "feature_columns = [\n",
    "    'amscHprsovDrivF-1a', 'amscHprsovDrivF-1b', 'amscHprsovDrivF-2b',\n",
    "    'amscPrsovDrivF-1a', 'amscPrsovDrivF-1b', 'amscPrsovDrivF-2b',\n",
    "    'basBleedLowPressF-1a', 'basBleedLowPressF-2b', 'basBleedLowTempF-1a',\n",
    "    'basBleedLowTempF-2b', 'basBleedOverPressF-1a', 'basBleedOverPressF-2b',\n",
    "    'basBleedOverTempF-1a', 'basBleedOverTempF-2b', 'bleedFavTmCmd-1a',\n",
    "    'bleedFavTmCmd-1b', 'bleedFavTmCmd-2a', 'bleedFavTmCmd-2b',\n",
    "    'bleedFavTmFbk-1a', 'bleedFavTmFbk-1b', 'bleedFavTmFbk-2b',\n",
    "    'bleedHprsovCmdStatus-1a', 'bleedHprsovCmdStatus-1b',\n",
    "    'bleedHprsovCmdStatus-2a', 'bleedHprsovCmdStatus-2b',\n",
    "    'bleedHprsovOpPosStatus-1a', 'bleedHprsovOpPosStatus-1b',\n",
    "    'bleedHprsovOpPosStatus-2a', 'bleedHprsovOpPosStatus-2b',\n",
    "    'bleedMonPress-1a', 'bleedMonPress-1b', 'bleedMonPress-2a',\n",
    "    'bleedMonPress-2b', 'bleedOnStatus-1a', 'bleedOnStatus-1b',\n",
    "    'bleedOnStatus-2b', 'bleedOverpressCas-2a', 'bleedOverpressCas-2b',\n",
    "    'bleedPrecoolDiffPress-1a', 'bleedPrecoolDiffPress-1b',\n",
    "    'bleedPrecoolDiffPress-2a', 'bleedPrecoolDiffPress-2b',\n",
    "    'bleedPrsovClPosStatus-1a', 'bleedPrsovClPosStatus-2a',\n",
    "    'bleedPrsovFbk-1a'\n",
    "]\n",
    "\n",
    "# Calcular média, mediana e desvio padrão para as colunas listadas\n",
    "mean_features = unique_df[feature_columns].mean().compute()\n",
    "median_features = unique_df[feature_columns].median().compute()\n",
    "std_features = unique_df[feature_columns].std().compute()\n",
    "\n",
    "# Criar um DataFrame com os resultados\n",
    "features_df = pd.DataFrame({\n",
    "    'mean': mean_features,\n",
    "    'median': median_features,\n",
    "    'std': std_features\n",
    "})\n",
    "\n",
    "print(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9bde92a-a145-402a-966d-3136bc22f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputar os valores NaN\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Treinar o modelo\n",
    "clf = LogisticRegression(max_iter=10000)\n",
    "clf.fit(X_train_imputed, y_train)\n",
    "\n",
    "# Obter a importância das características\n",
    "importance = clf.coef_[0]\n",
    "\n",
    "# Mapear a importância para os nomes das colunas\n",
    "feature_importance = dict(zip(X.columns, importance))\n",
    "\n",
    "# Agora, você pode filtrar as características com base em um limiar de importância\n",
    "importance_threshold = 0.05  # Ajuste conforme necessário\n",
    "important_features = [feature for feature, importance in feature_importance.items() if abs(importance) > importance_threshold]\n",
    "\n",
    "# Filtrar o DataFrame para manter apenas as características importantes\n",
    "filtered_df = unique_df[important_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adbd1a73-6c14-4902-ae02-72ccc5718096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bleedFavTmCmd-2a</th>\n",
       "      <th>bleedFavTmCmd-2b</th>\n",
       "      <th>bleedFavTmFbk-2b</th>\n",
       "      <th>bleedMonPress-1a</th>\n",
       "      <th>bleedMonPress-1b</th>\n",
       "      <th>bleedMonPress-2a</th>\n",
       "      <th>bleedMonPress-2b</th>\n",
       "      <th>bleedPrsovFbk-1a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: to_pyarrow_string, 109 graph layers</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "              bleedFavTmCmd-2a bleedFavTmCmd-2b bleedFavTmFbk-2b bleedMonPress-1a bleedMonPress-1b bleedMonPress-2a bleedMonPress-2b bleedPrsovFbk-1a\n",
       "npartitions=1                                                                                                                                        \n",
       "                       float64          float64          float64          float64          float64          float64          float64          float64\n",
       "                           ...              ...              ...              ...              ...              ...              ...              ...\n",
       "Dask Name: to_pyarrow_string, 109 graph layers"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbebbae-193d-4c89-895d-22ddf5080e48",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a136c83-71a8-401d-99c6-e1d68903b823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do Random Forest no conjunto de teste: 1.0000\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       falha       1.00      1.00      1.00       144\n",
      "      normal       1.00      1.00      1.00    342181\n",
      "\n",
      "    accuracy                           1.00    342325\n",
      "   macro avg       1.00      1.00      1.00    342325\n",
      "weighted avg       1.00      1.00      1.00    342325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Inicializar o classificador Random Forest\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Treinar o modelo usando os dados de treinamento imputados\n",
    "rf_clf.fit(X_train_imputed, y_train)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_rf = rf_clf.predict(X_test_imputed)\n",
    "\n",
    "# Avaliar o desempenho do modelo\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Acurácia do Random Forest no conjunto de teste: {accuracy_rf:.4f}\")\n",
    "\n",
    "# Mostrar relatório de classificação\n",
    "print(\"\\nRelatório de Classificação:\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbefe08-e744-4230-9871-d108fc19c462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
